{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from speos.preprocessing.handler import InputHandler\n",
    "from speos.utils.config import Config\n",
    "from speos.preprocessing.datasets import DatasetBootstrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.parse_yaml(\"config_uc_only_nohetio_film_newstorage.yaml\")\n",
    "prepro = InputHandler(config).get_preprocessor()\n",
    "prepro.build_graph(adjacency=False)\n",
    "data = prepro.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = DatasetBootstrapper(holdout_size=config.input.holdout_size, name=config.name, config=config).get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/mnt/storage/speos/results/uc_film_nohetioouter_results.json\", \"r\") as file:\n",
    "    results =  [key for key, value in json.load(file)[0].items() if value >= 11]\n",
    "\n",
    "indices = torch.LongTensor([prepro.hgnc2id[hgnc] for hgnc in results])\n",
    "\n",
    "with open(\"/mnt/storage/speos/results/uc_film_nohetioouter_results.json\", \"r\") as file:\n",
    "    results =  [key for key, value in json.load(file)[0].items() if value >= 1 and value < 11]\n",
    "\n",
    "indices_weak = torch.LongTensor([prepro.hgnc2id[hgnc] for hgnc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coregenes = dataset.data.y.long() \n",
    "coregenes[indices] = 1\n",
    "coregenes.sum()\n",
    "\n",
    "coregenes_weak = torch.zeros_like(coregenes)\n",
    "coregenes_weak[indices_weak] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsps = pd.read_csv(\"hsps/uc.txt\", header=None, index_col=None, sep=\"\\t\")\n",
    "hsp_indices = [prepro.hgnc2id[hgnc] for hgnc in hsps.iloc[:, 0]]\n",
    "new_y = torch.zeros_like(dataset.data.y)\n",
    "new_y[np.asarray(hsp_indices)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_zstat = pd.read_csv(\"data/gwas/UC.genes.out\", header=0, index_col=\"GENE\", usecols=[\"GENE\", \"ZSTAT\"], sep=\" \")\n",
    "gwas_zstat = gwas_zstat.loc[gwas_zstat.index.isin(prepro.entrez2id.keys()), :]\n",
    "len(gwas_zstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_zstat = gwas_zstat.rename(prepro.entrez2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gwas_zstat = torch.zeros_like(new_y)\n",
    "all_sign_indices = gwas_zstat[gwas_zstat[\"ZSTAT\"] > 5].index\n",
    "all_gwas_zstat[all_sign_indices] = 1\n",
    "\n",
    "all_gwas_zstat[all_gwas_zstat.logical_and(coregenes)] = 0\n",
    "all_gwas_zstat[all_gwas_zstat.logical_and(coregenes_weak)] = 0\n",
    "all_gwas_zstat.sum()\n",
    "\n",
    "# must be 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(data=dataset.data.x.numpy(), columns=prepro.get_feature_names()).rename(prepro.id2hgnc, axis=0)\n",
    "strongcore_features = features.iloc[coregenes.nonzero().squeeze().tolist(), :]\n",
    "weakcore_features = features.iloc[coregenes_weak.nonzero().squeeze().tolist(), :]\n",
    "hsp_features = features.iloc[new_y.nonzero().squeeze().tolist(), :]\n",
    "gwas_hsp_features = features.iloc[all_gwas_zstat.nonzero().squeeze().tolist(), :]\n",
    "peripheral_features = features.iloc[(1 - (coregenes + coregenes_weak + new_y)).nonzero().squeeze().tolist(), :]\n",
    "eligible_features = features.iloc[(1 - coregenes_weak).nonzero().squeeze().tolist(), :]\n",
    "eligible_features_weakcore = features.iloc[(1 - coregenes).nonzero().squeeze().tolist(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speos.visualization.settings import *\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# fragments = [Â [[0,1,2],[0.1, 0.2, 0.3]], [[4,5,6],[0.1, 0.2, 0.3]]\n",
    "\n",
    "def get_fragments(values, sign_list):\n",
    "    fragments = []\n",
    "    index = 0\n",
    "    fragment_idx = 0\n",
    "    for sign in sign_list.tolist():\n",
    "        if sign:\n",
    "            try:\n",
    "                fragments[fragment_idx][0].append(index)\n",
    "                fragments[fragment_idx][1].append(values[index])\n",
    "            except IndexError:\n",
    "                fragments.append([[index], [values[index]]])\n",
    "        else:\n",
    "            try:\n",
    "                fragments[fragment_idx]\n",
    "                fragment_idx += 1\n",
    "            except IndexError:\n",
    "                # if we already incremented dont increment again\n",
    "                pass\n",
    "        index += 1\n",
    "\n",
    "    return fragments\n",
    "        \n",
    "            \n",
    "\n",
    "def plot_enrichment_fisher(featurename, eligible_features, strongcore_features, peripheral_features, hsp_features, ax=None, fig=None):\n",
    "    core_enrichment = []\n",
    "    peripheral_enrichment = []\n",
    "    hsp_enrichment = []\n",
    "    core_pval = []\n",
    "    core_arrays = []\n",
    "    peripheral_pval = []\n",
    "    peripheral_arrays = []\n",
    "    hsp_pval = []\n",
    "    hsp_arrays = []\n",
    "\n",
    "    for i in range(100, -1, -1):\n",
    "      threshold = np.quantile(eligible_features[featurename], i / 100)\n",
    "      total_count = (eligible_features[featurename] >= threshold).sum()\n",
    "      core_count = (strongcore_features[featurename] >= threshold).sum()\n",
    "      peripheral_count = (peripheral_features[featurename] >= threshold).sum()\n",
    "      hsp_count = (hsp_features[featurename] >= threshold).sum()\n",
    "\n",
    "      array = np.asarray([[core_count, total_count-core_count],\n",
    "              [len(strongcore_features) - core_count, len(eligible_features) - len(strongcore_features) - total_count + core_count]])\n",
    "        \n",
    "      assert array[0, :].sum() == total_count\n",
    "      assert array[1, :].sum() == len(eligible_features) - total_count\n",
    "      assert array[:, 0].sum() == len(strongcore_features)\n",
    "      assert array[:, 1].sum() == len(eligible_features) - len(strongcore_features)\n",
    "\n",
    "      core_arrays.append(array.flatten())\n",
    "      core_enrichment.append(fisher_exact(array)[0])\n",
    "      core_pval.append(fisher_exact(array)[1])\n",
    "      \n",
    "      array = np.asarray([[peripheral_count, total_count-peripheral_count],\n",
    "              [len(peripheral_features) - peripheral_count, len(eligible_features) - len(peripheral_features) - total_count + peripheral_count]])\n",
    "        \n",
    "      assert array[0, :].sum() == total_count\n",
    "      assert array[1, :].sum() == len(eligible_features) - total_count\n",
    "      assert array[:, 0].sum() == len(peripheral_features)\n",
    "      assert array[:, 1].sum() == len(eligible_features) - len(peripheral_features)\n",
    "\n",
    "      peripheral_arrays.append(array.flatten())\n",
    "      peripheral_enrichment.append(fisher_exact(array)[0])\n",
    "      peripheral_pval.append(fisher_exact(array)[1])\n",
    "\n",
    "      array = np.asarray([[hsp_count, total_count-hsp_count],\n",
    "              [len(hsp_features) - hsp_count, len(eligible_features) - len(hsp_features) - total_count + hsp_count]])\n",
    "\n",
    "        \n",
    "      assert array[0, :].sum() == total_count\n",
    "      assert array[1, :].sum() == len(eligible_features) - total_count\n",
    "      assert array[:, 0].sum() == len(hsp_features)\n",
    "      assert array[:, 1].sum() == len(eligible_features) - len(hsp_features)\n",
    "\n",
    "      hsp_arrays.append(array.flatten())\n",
    "      hsp_enrichment.append(fisher_exact(array)[0])\n",
    "      hsp_pval.append(fisher_exact(array)[1])\n",
    "    \n",
    "    \n",
    "\n",
    "    core_enrichment = np.asarray(core_enrichment)\n",
    "    peripheral_enrichment = np.asarray(peripheral_enrichment)\n",
    "    hsp_enrichment = np.asarray(hsp_enrichment)\n",
    "    total_sign = fdrcorrection(core_pval + peripheral_pval + hsp_pval)[0]\n",
    "    total_fdr = fdrcorrection(core_pval + peripheral_pval + hsp_pval)[1]\n",
    "    core_sign = total_sign[:len(core_pval)]\n",
    "    core_fdr = total_fdr[:len(core_pval)]\n",
    "\n",
    "    peripheral_sign = total_sign[len(core_pval):-len(hsp_pval)]\n",
    "    peripheral_fdr = total_fdr[len(core_pval):-len(hsp_pval)]\n",
    "\n",
    "    hsp_sign = total_sign[-len(hsp_pval):]\n",
    "    hsp_fdr = total_fdr[-len(hsp_pval):]\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    for fragment in get_fragments(core_enrichment, core_sign):\n",
    "        ax.plot(fragment[0], fragment[1], color=\"#01016f\")\n",
    "    for fragment in get_fragments(peripheral_enrichment, peripheral_sign):\n",
    "        ax.plot(fragment[0], fragment[1], color=\"#5a5a5a\")\n",
    "    for fragment in get_fragments(hsp_enrichment, hsp_sign):\n",
    "        ax.plot(fragment[0], fragment[1], color=\"#d8031c\")\n",
    "\n",
    "\n",
    "    ax.plot(np.arange(101), core_enrichment, color=\"lightblue\", zorder=-5)\n",
    "    #ax.plot(np.arange(101)[peripheral_sign], peripheral_enrichment[peripheral_sign], color=\"#5a5a5a\")\n",
    "    ax.plot(np.arange(101), peripheral_enrichment, color=\"lightgray\", zorder=-5)\n",
    "    #ax.plot(np.arange(101)[hsp_sign], hsp_enrichment[hsp_sign], color=\"#d8031c\")\n",
    "    ax.plot(np.arange(101), hsp_enrichment, color=\"pink\", zorder=-5)\n",
    "    ax.set_xticks((0, 20, 40, 60, 80, 100))\n",
    "    ax.set_xticklabels((100, 80, 60, 40, 20, 0))\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_xlabel(\"Percentile ({})\".format(featurename),  fontsize=7)\n",
    "    ax.set_ylabel(\"Odds Ratio\", fontsize=7)\n",
    "    ax.hlines(1, 0, 100, colors=\"black\", linewidth=1)\n",
    "    ax.set_yscale(\"symlog\")\n",
    "    ax.set_yticks([ 1/10, 1/2, 1,2, 5, 10, 20])\n",
    "    ax.set_yticklabels([0.1, 1/2, 1, 2, 5, 10, 20])\n",
    "    ax.grid(which=\"major\", axis=\"y\", ls=\"--\", color=\"lightgray\", zorder=-5)\n",
    "\n",
    "    core_values = np.concatenate((core_arrays, core_enrichment.reshape(-1,1), np.asarray(core_pval).reshape(-1,1), core_fdr.reshape(-1,1)), axis=1)\n",
    "    periph_values = np.concatenate((peripheral_arrays, peripheral_enrichment.reshape(-1,1), np.asarray(peripheral_pval).reshape(-1,1), peripheral_fdr.reshape(-1,1)), axis=1)\n",
    "    hsp_values = np.concatenate((hsp_arrays, hsp_enrichment.reshape(-1,1), np.asarray(hsp_pval).reshape(-1,1), hsp_fdr.reshape(-1,1)), axis=1)\n",
    "\n",
    "    columns = []\n",
    "    for group in [\"Core\", \"Peripheral\", \"HSP\"]:\n",
    "        columns.extend([\"{} in P.\".format(group), \"not {} in P.\".format(group), \"{} not in P.\".format(group), \"not {} not in P.\".format(group), \"{} Odds Ratio\".format(group), \"{} p-value\".format(group), \"{} FDR\".format(group)])\n",
    "    \n",
    "    df = pd.DataFrame(data = np.concatenate((np.arange(100,-1,-1).reshape(-1,1), core_values, periph_values, hsp_values), axis=1),\n",
    "                      columns=[\"Percentile\"] + columns)\n",
    "    \n",
    "    return fig, ax, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with one tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, df = plot_enrichment_fisher('Small Intestine - Terminal Ileum',  eligible_features, strongcore_features, peripheral_features, gwas_hsp_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(full_width*cm, 7*cm), sharex=False, sharey=True)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (featurename, ax) in enumerate(zip((\"Cells - EBV-transformed lymphocytes\",\"Whole Blood\",\"Spleen\", \"Artery - Tibial\", 'Brain - Frontal Cortex (BA9)', 'Brain - Anterior cingulate cortex (BA24)'), axes)):\n",
    "     fig, ax, df = plot_enrichment_fisher(featurename,  eligible_features, strongcore_features, peripheral_features, hsp_features, ax, fig)\n",
    "     df.to_csv(featurename +\"_by_snp.tsv\", sep=\"\\t\", index=False)\n",
    "     if i % 3 != 0:\n",
    "          ax.set_ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"input_features_fisher.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(full_width*cm, 7*cm), sharex=False, sharey=True)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (featurename, ax) in enumerate(zip((\"Cells - EBV-transformed lymphocytes\",\"Whole Blood\",\"Spleen\", \"Artery - Tibial\", 'Brain - Frontal Cortex (BA9)', 'Brain - Anterior cingulate cortex (BA24)'), axes)):\n",
    "     fig, ax, df = plot_enrichment_fisher(featurename,  eligible_features, strongcore_features, peripheral_features, gwas_hsp_features, ax, fig)\n",
    "     df.to_csv(featurename +\"_zscore.tsv\", sep=\"\\t\", index=False)\n",
    "     if i % 3 != 0:\n",
    "          ax.set_ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"input_features_fisher_gwas.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(full_width*cm, 7*cm), sharex=False, sharey=True)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (featurename, ax) in enumerate(zip((\"Cells - EBV-transformed lymphocytes\",\"Whole Blood\",\"Small Intestine - Terminal Ileum\", \"Artery - Tibial\", 'Brain - Frontal Cortex (BA9)', 'Brain - Anterior cingulate cortex (BA24)'), axes)):\n",
    "     fig, ax, df = plot_enrichment_fisher(featurename,  eligible_features, strongcore_features, peripheral_features, gwas_hsp_features, ax, fig)\n",
    "     df.to_csv(featurename +\"_zscore.tsv\", sep=\"\\t\", index=False)\n",
    "     if i % 3 != 0:\n",
    "          ax.set_ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"input_features_fisher_gwas_intestine.svg\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
